{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Project 3 - Book 2: Web APIs & Classification\n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "Nutrino is a leading provider of nutrition related data services and analytics. As part of the data science team, we have been tasked to generate business insights curated from popular social media platforms. The company will be able to use that information to better understand customers & markets, enhance decision-making, and ultimately increase profitability.\n",
    "\n",
    "To do so, we will first be scrapping data from reddit and using classification models such as Logistic Regression and Naive Bayes to uncover patterns within 2 popular diets, Keto and Vegan. In developing this proof of concept, we want to be able to classify all available text data into business ready data, stengthening our core analytics product. \n",
    "\n",
    "We also hope to reveal previously unrecognised sub-trends that pertains to attitudes, lifestyles and buying behavior. With a better understanding of the population and their eating patterns, we will be able to advise our clients oh how they can launch their targeted marketing campaigns and improve the success of their products and programs.\n",
    "\n",
    "\n",
    "## Executive Summary\n",
    "\n",
    "As the data science team in Nutrino, we have been tasked to build a classifier to improve core product of the company, which is to provide nutrition related data services and analytics. We are also tasked to identify patterns on 2 currently trending diets, keto and vegan. \n",
    "\n",
    "Our classifier was successful in predicting at an above 90% accuracy score. We also identified patterns in the motivations and preferences of the 2 groups of subredditors, which will help determine the kind of customer engagement with teach group. \n",
    "\n",
    "\n",
    "\n",
    "## Notebooks:\n",
    "- [Data Scrapping and Cleaning](./book1_data_scrapping_cleaning.ipynb)\n",
    "- [EDA](./book2_eda.ipynb)\n",
    "- [Modeling and Recommendations](./book3_preprocesing_modeling_recommendations.ipynb)\n",
    "\n",
    "\n",
    "## Contents:\n",
    "- [Import Libraries](#Import-Libraries)\n",
    "- [Import Data](#Import-Data)\n",
    "- [Baseline Accuracy](#Baseline-Accuracy)\n",
    "- [Model Prep](#Model-Prep)\n",
    "- [Model Selection](#Model-Selection)\n",
    "- [Modeling Test Data](#Modeling-Test-Data)\n",
    "- [Classification Metrics](#Classification-Metrics)\n",
    "- [Recommendations](#Recommendations)\n",
    "- [Conclusion](#Conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../datasets/data_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1644, 8)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>vegan_label</th>\n",
       "      <th>subred_name</th>\n",
       "      <th>author</th>\n",
       "      <th>upvotes</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>post_id</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vegan hacktivists looking developers ui design...</td>\n",
       "      <td>1</td>\n",
       "      <td>r/vegan</td>\n",
       "      <td>veganactivismbot</td>\n",
       "      <td>76</td>\n",
       "      <td>0</td>\n",
       "      <td>f3svif</td>\n",
       "      <td>184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>last words fellow vegan elijah mcclain murdere...</td>\n",
       "      <td>1</td>\n",
       "      <td>r/vegan</td>\n",
       "      <td>VenmoMeFiveBucks</td>\n",
       "      <td>5114</td>\n",
       "      <td>409</td>\n",
       "      <td>hf6eej</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>promising future think</td>\n",
       "      <td>1</td>\n",
       "      <td>r/vegan</td>\n",
       "      <td>The_Shorey</td>\n",
       "      <td>2709</td>\n",
       "      <td>151</td>\n",
       "      <td>hfkmlc</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>vegan mean</td>\n",
       "      <td>1</td>\n",
       "      <td>r/vegan</td>\n",
       "      <td>benwaboekar</td>\n",
       "      <td>188</td>\n",
       "      <td>40</td>\n",
       "      <td>hfl3xp</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>suddenly finding smell cooking meat repulsive ...</td>\n",
       "      <td>1</td>\n",
       "      <td>r/vegan</td>\n",
       "      <td>sneakyt123</td>\n",
       "      <td>38</td>\n",
       "      <td>11</td>\n",
       "      <td>hfn5ku</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  vegan_label subred_name  \\\n",
       "0  vegan hacktivists looking developers ui design...            1     r/vegan   \n",
       "1  last words fellow vegan elijah mcclain murdere...            1     r/vegan   \n",
       "2                             promising future think            1     r/vegan   \n",
       "3                                         vegan mean            1     r/vegan   \n",
       "4  suddenly finding smell cooking meat repulsive ...            1     r/vegan   \n",
       "\n",
       "             author  upvotes  num_comments post_id  word_count  \n",
       "0  veganactivismbot       76             0  f3svif         184  \n",
       "1  VenmoMeFiveBucks     5114           409  hf6eej          13  \n",
       "2        The_Shorey     2709           151  hfkmlc           3  \n",
       "3       benwaboekar      188            40  hfl3xp           2  \n",
       "4        sneakyt123       38            11  hfn5ku          38  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text            0\n",
       "vegan_label     0\n",
       "subred_name     0\n",
       "author          0\n",
       "upvotes         0\n",
       "num_comments    0\n",
       "post_id         0\n",
       "word_count      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()\n",
    "#no null values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.587591\n",
       "0    0.412409\n",
       "Name: vegan_label, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's calculate a baseline score so that we know if \n",
    "#our model is outperforming our null model\n",
    "\n",
    "df['vegan_label'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpretation of baseline accuracy\n",
    "The baseline accuracy means that if we predict 1 for all posts, we would be right at least 57% of the time. \n",
    "\n",
    "Now let's prepare for modelling!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Prep: Create feature matrix (`X`) and target vector (`y`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X=df['text']\n",
    "y=df['vegan_label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Prep: Train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,stratify=y)\n",
    "#here we stratify due to the unbalanced classes in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Prep: Steps for Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transformer = [('cvec', CountVectorizer()),('tvec',TfidfVectorizer())]\n",
    "model = [('lr', LogisticRegression()),('nb', MultinomialNB())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trans_pipe_params = {\n",
    "    'cvec__max_features': [1000, 1500, 2000],\n",
    "    'cvec__min_df': [2, 3],\n",
    "    'cvec__max_df': [0.8, 0.85, 0.9],\n",
    "    'cvec__ngram_range': [(1,1), (1,2)],\n",
    "    'tvec__max_features': [2500, 3000, 3500],\n",
    "    'tvec__max_df': [0.3, 0.5, 0.7],\n",
    "    'tvec__sublinear_tf': [True, False],\n",
    "    'tvec__ngram_range': [(1,1), (1,2)]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "lower not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-e0c395254b67>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrans_pipe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrans_pipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fit_transform'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mlast_step\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlast_step\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mXt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1379\u001b[0m             \u001b[0mTf\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0midf\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mweighted\u001b[0m \u001b[0mdocument\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mterm\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1380\u001b[0m         \"\"\"\n\u001b[0;32m-> 1381\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1382\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1383\u001b[0m         \u001b[0;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m    867\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m--> 869\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m    870\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 792\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    793\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    794\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m             return lambda doc: self._word_ngrams(\n\u001b[0;32m--> 266\u001b[0;31m                 tokenize(preprocess(self.decode(doc))), stop_words)\n\u001b[0m\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlowercase\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    689\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetnnz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 691\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" not found\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: lower not found"
     ]
    }
   ],
   "source": [
    "trans_pipe = Pipeline(transformer)\n",
    "trans_pipe.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_pipe_params = {\n",
    "    'lr__penalty': ['l1', 'l2'],\n",
    "    'lr__solver' : ['liblinear'],\n",
    "    'lr__C':[1,0.1,0.01], \n",
    "    'nb__alpha' : [1.0, 1.1, 1.2],\n",
    "    'nb__fit_prior' : [True, False]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_pipe = Pipeline(model)\n",
    "opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#here we prepare the steps for pipeline\n",
    "# cvec_lr = [('cvec', CountVectorizer()),('lr', LogisticRegression())]\n",
    "# tvec_lr = [('tvec',TfidfVectorizer()),('lr', LogisticRegression())]\n",
    "# cvec_nb = [('cvec', CountVectorizer()),('nb', MultinomialNB())]\n",
    "# tvec_nb = [('tvec',TfidfVectorizer()),('nb', MultinomialNB())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choice of Model\n",
    "\n",
    "In order to select a model, we will be running 2 vectorizers and 2 models. \n",
    "The vectorizers are namely Count Vectorizer and TFIDF Vectorizer.\n",
    "Count Vectoriser counts the number of times a word appears. \n",
    "TFIDF goes a step further to apply a penalty for words that appear in  multiple posts. \n",
    "\n",
    "The models we chose are Logistic Regression and Naive Bayes (MultiNomial). \n",
    "\n",
    "Logistic Regression estimates the relationship between our features and the target variable by estimating probabilities using a sigmoid function.\n",
    "Naive bayes classifier is based on the bayes theorem which is in turn baed on conditional probabilities. It predicts probailities for each class and the class with the highest probability is ocnsidered the most likely class. \n",
    "\n",
    "We chose log reg because:\n",
    "- Y is binary\n",
    "- easy to interpret\n",
    "\n",
    "We chose NB because:\n",
    "- performs well with text classification\n",
    "- specifically the Multinomial because the columns of X are all integer counts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: CountVectorizer and Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Prep: Pipe_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipe_1_params = {\n",
    "    'cvec__max_features': [1000, 1500, 2000],\n",
    "    'cvec__min_df': [2, 3],\n",
    "    'cvec__max_df': [0.8, 0.85, 0.9],\n",
    "    'cvec__ngram_range': [(1,1), (1,2)],\n",
    "    'lr__penalty': ['l1', 'l2'],\n",
    "    'lr__solver' : ['liblinear'],\n",
    "    'lr__C':[1,0.1,0.01]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Fit and Score: Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9596958174904943\n",
      "{'cvec__max_df': 0.8, 'cvec__max_features': 1000, 'cvec__min_df': 2, 'cvec__ngram_range': (1, 2), 'lr__C': 0.01, 'lr__penalty': 'l2', 'lr__solver': 'liblinear'}\n",
      "CPU times: user 3min 52s, sys: 1.96 s, total: 3min 54s\n",
      "Wall time: 3min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#instantiate pipeline\n",
    "pipe_1 = Pipeline(cvec_lr)\n",
    "\n",
    "gs_1 = GridSearchCV(pipe_1,param_grid=pipe_1_params,cv=5)\n",
    "gs_1.fit(X_train,y_train)\n",
    "\n",
    "print(gs_1.best_score_)\n",
    "print(gs_1.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: Tfidf Vectorizer and Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Prep: Pipe_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipe_2_params = {\n",
    "    'tvec__max_features': [2500, 3000, 3500],\n",
    "    'tvec__max_df': [0.3, 0.5, 0.7],\n",
    "    'tvec__sublinear_tf': [True, False],\n",
    "    'tvec__ngram_range': [(1,1), (1,2)],\n",
    "    'lr__penalty': ['l1', 'l2'],\n",
    "    'lr__solver' : ['liblinear'],\n",
    "    'lr__C':[1,0.1,0.01]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Fit and Score: Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9589353612167301\n",
      "{'lr__C': 1, 'lr__penalty': 'l2', 'lr__solver': 'liblinear', 'tvec__max_df': 0.5, 'tvec__max_features': 3500, 'tvec__ngram_range': (1, 2), 'tvec__sublinear_tf': True}\n",
      "CPU times: user 4min 24s, sys: 4.24 s, total: 4min 28s\n",
      "Wall time: 4min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#instantiate pipeline\n",
    "pipe_2 = Pipeline(tvec_lr)\n",
    "\n",
    "gs_2 = GridSearchCV(pipe_2,param_grid=pipe_2_params,cv=5)\n",
    "gs_2.fit(X_train,y_train)\n",
    "\n",
    "print(gs_2.best_score_)\n",
    "print(gs_2.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3: CountVectorizer and Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Prep: Pipe_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipe_3_params = {\n",
    "    'cvec__max_features': [1000, 1500, 2000],\n",
    "    'cvec__min_df': [2, 3],\n",
    "    'cvec__max_df': [0.75, 0.8, 0.85],\n",
    "    'cvec__ngram_range': [(1,1), (1,2)],\n",
    "    'nb__alpha' : [1.0, 1.1, 1.2],\n",
    "    'nb__fit_prior' : [True, False]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Fit and Score: Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9505703422053232\n",
      "{'cvec__max_df': 0.75, 'cvec__max_features': 1000, 'cvec__min_df': 3, 'cvec__ngram_range': (1, 2), 'nb__alpha': 1.0, 'nb__fit_prior': True}\n",
      "CPU times: user 4min 22s, sys: 4.48 s, total: 4min 27s\n",
      "Wall time: 4min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#instantiate pipeline\n",
    "pipe_3 = Pipeline(cvec_nb)\n",
    "\n",
    "gs_3 = GridSearchCV(pipe_3,param_grid=pipe_3_params,cv=5)\n",
    "gs_3.fit(X_train,y_train)\n",
    "\n",
    "print(gs_3.best_score_)\n",
    "print(gs_3.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 4: Tfidf Vectorizer and Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Prep: Pipe_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipe_4_params = {\n",
    "    'tvec__max_features': [2500, 3000, 3500],\n",
    "    'tvec__max_df': [0.3, 0.5, 0.7],\n",
    "    'tvec__sublinear_tf': [True, False],\n",
    "    'tvec__ngram_range': [(1,1), (1,2)],\n",
    "    'nb__alpha' : [1.0, 1.1, 1.2],\n",
    "    'nb__fit_prior' : [True, False]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Fit and Score: Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9437262357414449\n",
      "{'nb__alpha': 1.0, 'nb__fit_prior': True, 'tvec__max_df': 0.5, 'tvec__max_features': 3500, 'tvec__ngram_range': (1, 2), 'tvec__sublinear_tf': False}\n",
      "CPU times: user 4min, sys: 3.21 s, total: 4min 3s\n",
      "Wall time: 4min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#instantiate pipeline\n",
    "pipe_4 = Pipeline(tvec_nb)\n",
    "\n",
    "gs_4 = GridSearchCV(pipe_4,param_grid=pipe_4_params,cv=5)\n",
    "gs_4.fit(X_train,y_train)\n",
    "\n",
    "print(gs_4.best_score_)\n",
    "print(gs_4.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scores recap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CVec and LR Score: 0.9596958174904943\n",
      "TFIDFVec and LR Score: 0.9589353612167301\n",
      "CVec and NB Score: 0.9505703422053232\n",
      "TFIDFVec and NB Score: 0.9437262357414449\n"
     ]
    }
   ],
   "source": [
    "print(f'CVec and LR Score: {gs_1.best_score_}')\n",
    "print(f'TFIDFVec and LR Score: {gs_2.best_score_}')\n",
    "print(f'CVec and NB Score: {gs_3.best_score_}')\n",
    "print(f'TFIDFVec and NB Score: {gs_4.best_score_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Observation on train scores:\n",
    "Overall, all our scores are pretty similar and way above our baseline score of 0.571. The default score used by gridsearch is the accuracy score. Our accuracy scores range from 0.945 to 0.959. This means that our model is doing a pretty great job of classifying text from the 2 subreddits. \n",
    "\n",
    "##### Model Selection:\n",
    "Based on the cross validation scores from all 4 models, we will proceed with the Count Vectorizer and Logistic Regression combination, which gave us the highest train accuracy score of 0.959. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9604863221884499"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_1.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "opti_cvec = CountVectorizer(max_df =0.8, max_features=1000, min_df=2, ngram_range=(1, 2))\n",
    "X_test_cvec = opti_cvec.fit_transform(X_test).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9787234042553191"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression(C=0.01, penalty='l2', solver='liblinear')\n",
    "lr.fit(X_train_cvec,y_train)\n",
    "lr.score(X_test_cvec,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Observation of test scores\n",
    "Our test score exceeded our train score with 0.964, with some variance from the train score. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpretation of coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    features      coef\n",
      "931    vegan  0.706615\n",
      "57    animal  0.189788\n",
      "    features      coef\n",
      "150    carbs -0.308900\n",
      "479     keto -0.746959\n"
     ]
    }
   ],
   "source": [
    "wd_features = gs_1.best_estimator_.named_steps['cvec'].get_feature_names()\n",
    "wd_coef = gs_1.best_estimator_.named_steps['lr'].coef_[0]\n",
    "\n",
    "coef_df = pd.DataFrame(zip(wd_features,wd_coef),columns=['features','coef']).sort_values('coef',ascending=False)\n",
    "\n",
    "print(coef_df.head(2))\n",
    "print(coef_df.tail(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A positive coefficient points towards the vegan class while a negative coefficient points to the keto class. \n",
    "\n",
    "Besides the obvious words of vegan and keto, we can interpret the coefficient as follows:\n",
    "- animal -> As the word count for animal increases by 1, our classifier is e^0.434 = 1.54 times more likely to classify it as a vegan post\n",
    "- carbs ->  As the word count for carbs increases by 1, our classifier is e^0.766 = 2.15 times more likely to classify it as a vegan post"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds = gs_1.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Negative: 125\n",
      "False Positive (Type I): 11\n",
      "False Negative (Type II): 2\n",
      "True Positive: 191\n"
     ]
    }
   ],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_test,preds).ravel()\n",
    "print('True Negative:' ,tn)\n",
    "print('False Positive (Type I):' ,fp)\n",
    "print('False Negative (Type II):' ,fn)\n",
    "print('True Positive:' ,tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9455445544554455\n"
     ]
    }
   ],
   "source": [
    "#Precision\n",
    "\n",
    "prec = tp/(tp+fp)\n",
    "\n",
    "print(f'Precision: {prec}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precision\n",
    "Precision measures primarily on type I errors or false positives. This means that the model classifies a post as a vegan post when it is actually a keto post.\n",
    "\n",
    "Our precision score is 0.94 and this means that our model rarely mislabels a keto post as a vegan post. \n",
    "\n",
    "The shortcoming of this is that it cannot measure false negatives, classifying keto posts as vegan posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensitivity: 0.9896373056994818\n"
     ]
    }
   ],
   "source": [
    "#Recall/Sensitivity\n",
    "\n",
    "sens = tp/(tp+fn)\n",
    "\n",
    "print(f'Sensitivity: {sens}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recall/Sensitivity\n",
    "Recall focuses on type II errors. A type II error means that the model classifies a post as a keto post when it is actually a vegan post.\n",
    "\n",
    "Our precision score is 1.0 and this means that our model never mislabels a vegan post as a keto post. \n",
    "\n",
    "Vice versa, the shortcoming of this metric is that it cannot measure false positives. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9543774763791527"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's take a look at ROC AUC Score yet another metric\n",
    "roc_auc_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ROC AUC Score\n",
    "Our high ROC AUC score of 0.96 (close to 1) confirms that we have a good separation between our vegan and keto classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.92      0.95       136\n",
      "          1       0.95      0.99      0.97       193\n",
      "\n",
      "avg / total       0.96      0.96      0.96       329\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F1 Score\n",
    "The F1 score shows the balance between precision and recall. It is calculated by 2(precision*recall)/(precision+recall).\n",
    "Our high F1 score of 0.96 shows that we have a good balance of both of the above. \n",
    "\n",
    "Ultimately, we measure our success on the F1 score which generally works well with unbalanced classes, which we have at 58/42. \n",
    "Also, we dont particularly value identifying either false positive or false negatives more, Accuracy and F1 scores should be sufficient to measure our success. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our reccomendations are: \n",
    "- further develop the classifier to continuously improve our core product of generating data analytics  \n",
    "\n",
    "With this proof of concept, we can invest further resources to develop the model to eventually classify all available data, to contribute to our core product of nutrition data related services.  \n",
    "\n",
    "- vegan insights: focus on educating the target market\n",
    "\n",
    "With vegan subredditors, we noticed that the users were highly discerning. Afterall, vegans have made a life changing commitment to the lifestlye to avoid meat and animal products. They are concerned with making the world a better place. In order to rally these users, frequently post about the science behind veganism, share insights on how they can contribute to reduce animal cruelty and keep them updated on the change that veganism is bringing.\n",
    "\n",
    "- keto insights: focus on building a strong community\n",
    "\n",
    "With keto subredditors, a common goal/motivation is weight loss. Build a community with these people, share progress, recipes and tips. Build a platform when people can freely share their thoughts and concerns. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "There were 2 keys questions that we set out to answer in our problem statement:\n",
    "1. Can we build a classifier, with at least 90% accuracy, to sort text data from Keto and Vegan subreddits?\n",
    "2. Can we draw insights from these text data for targeted marketing?\n",
    "\n",
    "After data cleaning, feature engineering and EDA, we ran our text data through a couple of vectorizer-model combinations. We tuned the hyperparameters to generate the best scores. From there, we scored them on the training data, using the accuracy scores. We chose the model with the highest accuracy score and scored our optimal model with the test data. Finally, our classifier achieved a test score of 0.964, which means that it correctly classified text data 96.4% of the time.\n",
    "\n",
    "We also drew insights from the EDA and profiled our target market. We realised differences in motivations and preferences between the 2 groups. Our recommendations are tailored to each group with a focus on educating for the vegans and building a strong community for the keto crowd. \n",
    "\n",
    "With these, we will be able to provide better insights to our clients to help with their marketing campaigns. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To further improve our model, I believe that we could have included texts from the comment section which were not availabble via the API. We could perhaps explore using PRAW to scrap comments. \n",
    "\n",
    "We could also run sentiment analysis on the data to see which posts and comments were favoured by different users of the subreddits. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
