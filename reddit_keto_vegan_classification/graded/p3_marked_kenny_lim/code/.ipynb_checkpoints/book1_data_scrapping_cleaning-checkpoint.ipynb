{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3 - Book 1: Web APIs & Classification\n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "Nutrino is a leading provider of nutrition related data services and analytics. As part of the data science team, we have been tasked to generate business insights curated from popular social media platforms. The company will be able to use that information to better understand customers & markets, enhance decision-making, and ultimately increase profitability.\n",
    "\n",
    "To do so, we will first be scrapping data from reddit and using classification models such as Logistic Regression and Naive Bayes to uncover patterns within 2 popular diets, Keto and Vegan. We will measure our success using several classificationmetrics inclusing accuracy and F1 score. \n",
    "\n",
    "We hope to reveal previously unrecognised sub-trends that pertains to attitudes, lifestyles and buying behavior, strong sub trends as opposed to passing sub trends. With a better understanding of the population and their eating patterns, our clients will be able to strengthen their targeted marketing campaigns and improve the success of their products and programs.\n",
    "\n",
    "## Executive Summary\n",
    "\n",
    "As the data science team in Nutrino, we have been tasked to build a classifier to improve core product of the company, which is to provide nutrition related data services and analytics. We are also tasked to identify patterns on 2 currently trending diets, keto and vegan. \n",
    "\n",
    "Our classifier was successful in predicting at an above 90% accuracy score. We also identified patterns in the motivations and preferences of the 2 groups of subredditors, which will help determine the kind of customer engagement with teach group. \n",
    "\n",
    "\n",
    "## Notebooks:\n",
    "- [Data Scrapping and Cleaning](./book1_data_scrapping_cleaning.ipynb)\n",
    "- [EDA](./book2_eda.ipynb)\n",
    "- [Modeling and Recommendations](./book3_preprocesing_modeling_recommendations.ipynb)\n",
    "\n",
    "\n",
    "## Contents:\n",
    "- [Import Libraries](#Import-Libraries)\n",
    "- [Data Scrapping](#Data-Scrapping)\n",
    "- [Data Cleaning](#Data-Cleaning)\n",
    "- [Save Data to CSV](#Save-Data-to-CSV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import random\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Scrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#set header so that reddit wont think we are a bot and block us\n",
    "header = {'User-agent':'ididitforthemulz'}\n",
    "\n",
    "#list of subreddits that we want to scrap\n",
    "sub_reds = ['vegan','keto']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "https://old.reddit.com/r/vegan.json\n",
      "9\n",
      "1\n",
      "https://old.reddit.com/r/vegan.json?after=t3_hfq71l\n",
      "2\n",
      "2\n",
      "https://old.reddit.com/r/vegan.json?after=t3_hf1id4\n",
      "25\n",
      "3\n",
      "https://old.reddit.com/r/vegan.json?after=t3_hfo6rh\n",
      "8\n",
      "4\n",
      "https://old.reddit.com/r/vegan.json?after=t3_hfe2ej\n",
      "21\n",
      "5\n",
      "https://old.reddit.com/r/vegan.json?after=t3_hevwpu\n",
      "29\n",
      "6\n",
      "https://old.reddit.com/r/vegan.json?after=t3_hfdeu4\n",
      "3\n",
      "7\n",
      "https://old.reddit.com/r/vegan.json?after=t3_helm4l\n",
      "7\n",
      "8\n",
      "https://old.reddit.com/r/vegan.json?after=t3_hetne6\n",
      "19\n",
      "9\n",
      "https://old.reddit.com/r/vegan.json?after=t3_herhpc\n",
      "10\n",
      "10\n",
      "https://old.reddit.com/r/vegan.json?after=t3_hejv4u\n",
      "18\n",
      "11\n",
      "https://old.reddit.com/r/vegan.json?after=t3_hed1ew\n",
      "4\n",
      "12\n",
      "https://old.reddit.com/r/vegan.json?after=t3_hepimm\n",
      "3\n",
      "13\n",
      "https://old.reddit.com/r/vegan.json?after=t3_hdpv1d\n",
      "8\n",
      "14\n",
      "https://old.reddit.com/r/vegan.json?after=t3_he121a\n",
      "6\n",
      "15\n",
      "https://old.reddit.com/r/vegan.json?after=t3_hdtbxm\n",
      "28\n",
      "16\n",
      "https://old.reddit.com/r/vegan.json?after=t3_hdyrl4\n",
      "18\n",
      "17\n",
      "https://old.reddit.com/r/vegan.json?after=t3_he0hwz\n",
      "2\n",
      "18\n",
      "https://old.reddit.com/r/vegan.json?after=t3_hdv6kc\n",
      "13\n",
      "19\n",
      "https://old.reddit.com/r/vegan.json?after=t3_hduvxb\n",
      "23\n",
      "20\n",
      "https://old.reddit.com/r/vegan.json?after=t3_hdhtmi\n",
      "12\n",
      "21\n",
      "https://old.reddit.com/r/vegan.json?after=t3_hdj25w\n",
      "9\n",
      "22\n",
      "https://old.reddit.com/r/vegan.json?after=t3_hdb4mw\n",
      "30\n",
      "23\n",
      "https://old.reddit.com/r/vegan.json?after=t3_hda4j8\n",
      "4\n",
      "24\n",
      "https://old.reddit.com/r/vegan.json?after=t3_hd03st\n",
      "4\n",
      "25\n",
      "https://old.reddit.com/r/vegan.json?after=t3_hd5cwz\n",
      "2\n",
      "26\n",
      "https://old.reddit.com/r/vegan.json?after=t3_hd76my\n",
      "22\n",
      "27\n",
      "https://old.reddit.com/r/vegan.json?after=t3_hc9omu\n",
      "21\n",
      "28\n",
      "https://old.reddit.com/r/vegan.json?after=t3_hckdc2\n",
      "24\n",
      "29\n",
      "https://old.reddit.com/r/vegan.json?after=t3_hct5fp\n",
      "14\n",
      "30\n",
      "https://old.reddit.com/r/vegan.json?after=t3_hc1hbm\n",
      "12\n",
      "31\n",
      "https://old.reddit.com/r/vegan.json?after=t3_hc4cal\n",
      "26\n",
      "32\n",
      "https://old.reddit.com/r/vegan.json?after=t3_hc8cg6\n",
      "3\n",
      "33\n",
      "https://old.reddit.com/r/vegan.json?after=t3_hbq09p\n",
      "24\n",
      "34\n",
      "https://old.reddit.com/r/vegan.json?after=t3_hbu19c\n",
      "15\n",
      "35\n",
      "https://old.reddit.com/r/vegan.json?after=t3_hbsb6v\n",
      "15\n",
      "36\n",
      "https://old.reddit.com/r/vegan.json?after=t3_hbj304\n",
      "29\n",
      "37\n",
      "https://old.reddit.com/r/vegan.json?after=t3_hbibrh\n",
      "21\n",
      "38\n",
      "https://old.reddit.com/r/vegan.json?after=t3_hbrl18\n",
      "15\n",
      "39\n",
      "https://old.reddit.com/r/vegan.json\n",
      "8\n",
      "40\n",
      "https://old.reddit.com/r/vegan.json?after=t3_hfr8qo\n",
      "3\n",
      "41\n",
      "https://old.reddit.com/r/vegan.json?after=t3_hfcw5u\n",
      "12\n",
      "0\n",
      "https://old.reddit.com/r/keto.json?after=t3_hf13k2\n",
      "23\n",
      "1\n",
      "https://old.reddit.com/r/keto.json\n",
      "17\n",
      "2\n",
      "https://old.reddit.com/r/keto.json?after=t3_hfouhf\n",
      "23\n",
      "3\n",
      "https://old.reddit.com/r/keto.json?after=t3_hf2fnn\n",
      "14\n",
      "4\n",
      "https://old.reddit.com/r/keto.json?after=t3_hfd8c0\n",
      "19\n",
      "5\n",
      "https://old.reddit.com/r/keto.json?after=t3_hezt4v\n",
      "3\n",
      "6\n",
      "https://old.reddit.com/r/keto.json?after=t3_hekwns\n",
      "21\n",
      "7\n",
      "https://old.reddit.com/r/keto.json?after=t3_hej7tm\n",
      "26\n",
      "8\n",
      "https://old.reddit.com/r/keto.json?after=t3_hefzuc\n",
      "2\n",
      "9\n",
      "https://old.reddit.com/r/keto.json?after=t3_he0i4z\n",
      "10\n",
      "10\n",
      "https://old.reddit.com/r/keto.json?after=t3_hdjbe4\n",
      "26\n",
      "11\n",
      "https://old.reddit.com/r/keto.json?after=t3_hd14us\n",
      "14\n",
      "12\n",
      "https://old.reddit.com/r/keto.json?after=t3_hdcmit\n",
      "27\n",
      "13\n",
      "https://old.reddit.com/r/keto.json?after=t3_hd68iq\n",
      "8\n",
      "14\n",
      "https://old.reddit.com/r/keto.json?after=t3_hcshm0\n",
      "15\n",
      "15\n",
      "https://old.reddit.com/r/keto.json?after=t3_hc0edm\n",
      "10\n",
      "16\n",
      "https://old.reddit.com/r/keto.json?after=t3_hc510h\n",
      "21\n",
      "17\n",
      "https://old.reddit.com/r/keto.json?after=t3_hbwepq\n",
      "3\n",
      "18\n",
      "https://old.reddit.com/r/keto.json?after=t3_hbqwk9\n",
      "14\n",
      "19\n",
      "https://old.reddit.com/r/keto.json?after=t3_hbcgbn\n",
      "3\n",
      "20\n",
      "https://old.reddit.com/r/keto.json?after=t3_hb6cek\n",
      "23\n",
      "21\n",
      "https://old.reddit.com/r/keto.json?after=t3_hb284n\n",
      "19\n",
      "22\n",
      "https://old.reddit.com/r/keto.json?after=t3_hamh6p\n",
      "25\n",
      "23\n",
      "https://old.reddit.com/r/keto.json?after=t3_ha5rfl\n",
      "23\n",
      "24\n",
      "https://old.reddit.com/r/keto.json?after=t3_ha6nx6\n",
      "23\n",
      "25\n",
      "https://old.reddit.com/r/keto.json?after=t3_h9z1pv\n",
      "9\n",
      "26\n",
      "https://old.reddit.com/r/keto.json?after=t3_h9be4x\n",
      "22\n",
      "27\n",
      "https://old.reddit.com/r/keto.json?after=t3_h99dhk\n",
      "17\n",
      "28\n",
      "https://old.reddit.com/r/keto.json?after=t3_h93x3q\n",
      "2\n",
      "29\n",
      "https://old.reddit.com/r/keto.json\n",
      "15\n",
      "30\n",
      "https://old.reddit.com/r/keto.json?after=t3_hfouhf\n",
      "18\n",
      "31\n",
      "https://old.reddit.com/r/keto.json?after=t3_hf2fnn\n",
      "5\n",
      "32\n",
      "https://old.reddit.com/r/keto.json?after=t3_hfd8c0\n",
      "15\n",
      "33\n",
      "https://old.reddit.com/r/keto.json?after=t3_hezt4v\n",
      "28\n",
      "34\n",
      "https://old.reddit.com/r/keto.json?after=t3_hekwns\n",
      "18\n",
      "35\n",
      "https://old.reddit.com/r/keto.json?after=t3_hej7tm\n",
      "10\n",
      "36\n",
      "https://old.reddit.com/r/keto.json?after=t3_hefzuc\n",
      "14\n",
      "37\n",
      "https://old.reddit.com/r/keto.json?after=t3_he0i4z\n",
      "6\n",
      "38\n",
      "https://old.reddit.com/r/keto.json?after=t3_hdjbe4\n",
      "26\n",
      "39\n",
      "https://old.reddit.com/r/keto.json?after=t3_hd14us\n",
      "2\n",
      "40\n",
      "https://old.reddit.com/r/keto.json?after=t3_hdcmit\n",
      "30\n",
      "41\n",
      "https://old.reddit.com/r/keto.json?after=t3_hd68iq\n",
      "18\n"
     ]
    }
   ],
   "source": [
    "#scrap vegan and keto posts \n",
    "all_posts = []\n",
    "after = None\n",
    "\n",
    "for sub in sub_reds:\n",
    "    url = f'https://old.reddit.com/r/{sub}.json'\n",
    "    posts = []\n",
    "    \n",
    "    for a in range(42):\n",
    "        print(a)\n",
    "        if after == None:\n",
    "            current_url = url\n",
    "        else:\n",
    "            current_url = url + '?after=' + after\n",
    "        print(current_url)\n",
    "        res = requests.get(current_url, headers=header)\n",
    "\n",
    "        if res.status_code != 200:\n",
    "            print('Status error', res.status_code)\n",
    "            break\n",
    "\n",
    "        current_dict = res.json()\n",
    "        current_posts = [p['data'] for p in current_dict['data']['children']]\n",
    "        posts.extend(current_posts)\n",
    "        after = current_dict['data']['after']\n",
    "\n",
    "        # generate a random sleep duration to look more 'natural'\n",
    "        sleep_duration = random.randint(2,30)\n",
    "        print(sleep_duration)\n",
    "        time.sleep(sleep_duration)\n",
    "        \n",
    "    all_posts.append(posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vegan_posts = []\n",
    "keto_posts = []\n",
    "subred_name = []\n",
    "author_lst = []\n",
    "ups_lst = []\n",
    "downs_lst = []\n",
    "num_comments_lst = []\n",
    "id_lst = []\n",
    "\n",
    "\n",
    "for i in range(2):\n",
    "    #identifying vegan posts\n",
    "    for post in all_posts[i]:\n",
    "        if i==0:\n",
    "            #Feature Engineering: here we add the text from \n",
    "            #selftext (which is also the body) to the title\n",
    "            vegan_posts.append(post['title'] + \" \" + post['selftext'])\n",
    "            \n",
    "            #this contains the username of the person that uploaded the post\n",
    "            author_lst.append(post['author'])\n",
    "            \n",
    "            #these are our labels\n",
    "            subred_name.append(post['subreddit_name_prefixed'])\n",
    "            \n",
    "            #number of upvotes\n",
    "            ups_lst.append(post['ups'])\n",
    "            \n",
    "            #number of downvotes\n",
    "            downs_lst.append(post['downs'])\n",
    "            \n",
    "            #number of comments\n",
    "            num_comments_lst.append(post['num_comments'])\n",
    "            \n",
    "            #id of post\n",
    "            id_lst.append(post['id'])\n",
    "            \n",
    "        #we do the same for keto posts\n",
    "        else:\n",
    "            keto_posts.append(post['title'] + \" \" + post['selftext'])\n",
    "            author_lst.append(post['author'])\n",
    "            subred_name.append(post['subreddit_name_prefixed'])\n",
    "            ups_lst.append(post['ups'])\n",
    "            downs_lst.append(post['downs'])\n",
    "            num_comments_lst.append(post['num_comments'])\n",
    "            id_lst.append(post['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1050 1017\n",
      "2067 2067 2067 2067 2067 2067\n"
     ]
    }
   ],
   "source": [
    "#check the length of all our lists\n",
    "\n",
    "#vegan and keto posts have been split into 2 lists\n",
    "print(len(vegan_posts),len(keto_posts))\n",
    "\n",
    "#the rest of the data are all in 1 list\n",
    "print(len(subred_name),len(author_lst),len(ups_lst),len(downs_lst),\n",
    "     len(num_comments_lst),len(id_lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vegan Hacktivists are looking for Developers, UI Designers, Writers and Social Media experts! ❤️ 🐮 Hi folks!\n",
      "\n",
      "The [Vegan Hacktivists](https://veganhacktivists.org/) is a small group of vegan activists (a few from our mod team here) that are working on several projects helping out organizations like The Save Movement, Meat The Victims, Planet Vegan, and more. We're currently looking for several different vegans to fill volunteer positions to help us spread veganism through online activism.\n",
      "\n",
      "🐮 **Developers:** We're looking for developers that have experience in Laravel, PHP, CSS and JS, and are familiar (or can get familiar with) with Github, Trello and Discord. If you're interested, [apply here](https://veganhacktivists.org/apply/developers)!\n",
      "\n",
      "🐷 **UI Designers:** We're looking for designers that have experience with making UI designs for websites using tools like Figma, Sketch, and other collaborative UI designing tools. if you're interested, [apply here](http://veganhacktivists.org/apply/designers)!\n",
      "\n",
      "🐰 **Graphic Designers:** We're looking for designers that have experience with making logo's, banners, social media posts, etc, using tools like Photoshop for projects/organizations. Shoot us your portfolio if you're interested, [here](http://veganhacktivists.org/apply/designers)!\n",
      "\n",
      "🐭 **Writers:** We're currently in need of folks that can write content for both our [r/vegan](https://www.reddit.com/r/vegan/) wiki redesign and our vegan challenge website, that have the ability to work with other writers and have better grammar than this sentence I'm constructing! Contact us [here](mailto:hello@veganhacktivists.org).\n",
      "\n",
      "🦊 **Social Media Manager:** We're currently looking for one to two people that can help manage our social media profiles so that we can just focus on our projects and activism, mainly Instagram. If interested, contact us [here](mailto:hello@veganhacktivists.org).\n",
      "\n",
      "If you've ever been interested in vegan activism but have yet to find something that fits you, I encourage you to apply above if you have the time available to do so! If none of these things fit you but you'd still like to volunteer your time, please check out [this link](https://veganactivism.org/) and you'll see a list of organizations available looking for your help! 💕\n",
      "\n",
      "Thank you!\n",
      "[2020-06-24] - [What's Your Question Wednesday] – Have a question? So does everyone else! Hey /r/keto!\n",
      "\n",
      "We all have questions. Questions about Ketogenic foods, how our bodies react to ketosis, what we can and can't do, how much of whatever to take in, whether or not we're missing something. We know you've got 'em - so, give 'em to us!\n",
      "\n",
      "Ask your questions here to help yourself and everyone else who's in your situation and might benefit! And conversely, all you /r/keto veterans - let's do our best to help people here. There are no dumb questions!\n",
      "\n",
      "If you're new to /r/keto and need some info, start with [Keto in a Nutshell](https://www.reddit.com/r/keto/wiki/keto_in_a_nutshell) and [the FAQ](https://www.reddit.com/r/keto/wiki/faq). Or, if you have a question that doesn't seem to be covered, head on over to the Community Support thread (pinned to the top of the subreddit) and ask the community!\n"
     ]
    }
   ],
   "source": [
    "print(vegan_posts[0])\n",
    "print(keto_posts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### First observations:\n",
    "- Now we have all our data from the subreddits\n",
    "- There are html syntax observed. We need to remove it.\n",
    "- We will links as well\n",
    "- We will also convert the data to lowercase and remove stop words\n",
    "- and finally, we will lemmatise the words instead of stemming which is a harsher method and could produce some non-words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have collected our data, we will move to cleaning the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#define a method to preprocess data\n",
    "def robust_text_preprocessing(text):\n",
    "    #change to lower\n",
    "    text=text.lower()\n",
    "    \n",
    "    #remove links\n",
    "    text=re.sub(r'http\\S+',\"\",text)\n",
    "    \n",
    "    #remove punctuation\n",
    "    text=re.sub(r\"[^A-Za-z0-9]\",\" \",text)\n",
    "    \n",
    "    #lemmatize\n",
    "    lemmatizer=WordNetLemmatizer()\n",
    "    text=lemmatizer.lemmatize(text)\n",
    "    \n",
    "    #remove stopwords\n",
    "    words=text.split()\n",
    "    text = [word for word in words if word not in stopwords.words('english')]\n",
    "    \n",
    "    #join words\n",
    "    text=(\" \".join(text))\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning and parsing the Vegan posts...\n",
      "Review 100 of 1050.\n",
      "Review 200 of 1050.\n",
      "Review 300 of 1050.\n",
      "Review 400 of 1050.\n",
      "Review 500 of 1050.\n",
      "Review 600 of 1050.\n",
      "Review 700 of 1050.\n",
      "Review 800 of 1050.\n",
      "Review 900 of 1050.\n",
      "Review 1000 of 1050.\n",
      "Cleaning and parsing the Keto posts...\n",
      "Review 100 of 1017.\n",
      "Review 200 of 1017.\n",
      "Review 300 of 1017.\n",
      "Review 400 of 1017.\n",
      "Review 500 of 1017.\n",
      "Review 600 of 1017.\n",
      "Review 700 of 1017.\n",
      "Review 800 of 1017.\n",
      "Review 900 of 1017.\n",
      "Review 1000 of 1017.\n",
      "CPU times: user 23.9 s, sys: 4.54 s, total: 28.4 s\n",
      "Wall time: 28.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "vegan_clean=[]\n",
    "print(\"Cleaning and parsing the Vegan posts...\")\n",
    "\n",
    "j = 0\n",
    "for str in vegan_posts:\n",
    "    # Convert posts to words, then append to list.\n",
    "    vegan_clean.append(robust_text_preprocessing(str))\n",
    "    \n",
    "    # If the index is divisible by 1000, print a message\n",
    "    if (j + 1) % 100 == 0:\n",
    "        print(f'Review {j + 1} of {len(vegan_posts)}.')\n",
    "    \n",
    "    j += 1\n",
    "\n",
    "# Let's do the same for our Keto data.\n",
    "keto_clean=[]\n",
    "print(\"Cleaning and parsing the Keto posts...\")\n",
    "\n",
    "j = 0\n",
    "for str in keto_posts:\n",
    "    # Convert posts to words, then append to list.\n",
    "    keto_clean.append(robust_text_preprocessing(str))\n",
    "    \n",
    "    # If the index is divisible by 1000, print a message\n",
    "    if (j + 1) % 100 == 0:\n",
    "        print(f'Review {j + 1} of {len(keto_posts)}.')\n",
    "    \n",
    "    j += 1"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## MARKING COMMENTS\n",
    "Be careful of using str (if you noticed it's in green) as variable, because it's an existing python method. Might have unintended consequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vegan hacktivists looking developers ui designers writers social media experts hi folks vegan hacktivists small group vegan activists mod team working several projects helping organizations like save movement meat victims planet vegan currently looking several different vegans fill volunteer positions help us spread veganism online activism developers looking developers experience laravel php css js familiar get familiar github trello discord interested apply ui designers looking designers experience making ui designs websites using tools like figma sketch collaborative ui designing tools interested apply graphic designers looking designers experience making logo banners social media posts etc using tools like photoshop projects organizations shoot us portfolio interested writers currently need folks write content r vegan wiki redesign vegan challenge website ability work writers better grammar sentence constructing contact us mailto hello veganhacktivists org social media manager currently looking one two people help manage social media profiles focus projects activism mainly instagram interested contact us mailto hello veganhacktivists org ever interested vegan activism yet find something fits encourage apply time available none things fit still like volunteer time please check link see list organizations available looking help thank\n",
      "2020 06 24 question wednesday question everyone else hey r keto questions questions ketogenic foods bodies react ketosis much whatever take whether missing something know got em give em us ask questions help everyone else situation might benefit conversely r keto veterans let best help people dumb questions new r keto need info start keto nutshell faq question seem covered head community support thread pinned top subreddit ask community\n"
     ]
    }
   ],
   "source": [
    "#lets take a look at our cleaned data\n",
    "print(vegan_clean[0])\n",
    "print(keto_clean[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage, we have completed our data collection and preprocessing. \n",
    "We now have 4 lists of posts:\n",
    "1. vegan_clean -> cleaned vegan posts without links\n",
    "2. keto_clean -> cleaned keto posts without links\n",
    "3. vegan_clean_links -> cleaned vegan posts without links\n",
    "4. keto_clean_links -> cleaned keto posts without links\n",
    "\n",
    "Let's move on to feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1050\n",
      "[1, 1, 1, 1, 1]\n",
      "1017\n",
      "[0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "#here we create a label for individual df for each subreddit\n",
    "vegan_label = [1 for str in vegan_posts]\n",
    "print(len(vegan_label))\n",
    "print(vegan_label[:5])\n",
    "keto_label = [0 for str in keto_posts]\n",
    "print(len(keto_label))\n",
    "print(keto_label[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#lets convert our data into dataframes\n",
    "vegan_df = pd.DataFrame(zip(vegan_clean,vegan_label),\n",
    "                        columns = ['text','vegan_label'])\n",
    "\n",
    "keto_df = pd.DataFrame(zip(keto_clean,keto_label),\n",
    "                        columns = ['text','vegan_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    1050\n",
       "0    1017\n",
       "Name: vegan_label, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lets see all the text data in one df\n",
    "text_df = pd.concat([vegan_df,keto_df],axis=0).reset_index()\n",
    "text_df.drop(columns='index',inplace=True)\n",
    "text_df['vegan_label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2067, 6)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lets put all other data in a df\n",
    "other_data_df = pd.DataFrame(zip(subred_name,author_lst,ups_lst,\n",
    "                                 downs_lst,num_comments_lst,id_lst),\n",
    "                            columns=['subred_name','author','upvotes',\n",
    "                                     'downvotes','num_comments','post_id'])\n",
    "other_data_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    1050\n",
      "0    1017\n",
      "Name: vegan_label, dtype: int64\n",
      "r/vegan    1050\n",
      "r/keto     1017\n",
      "Name: subred_name, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#let's see it all together\n",
    "all_data_df = pd.concat([text_df, other_data_df], axis=1)\n",
    "\n",
    "#double check that we can concat properly\n",
    "print(all_data_df['vegan_label'].value_counts())\n",
    "print(all_data_df['subred_name'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>vegan_label</th>\n",
       "      <th>subred_name</th>\n",
       "      <th>author</th>\n",
       "      <th>upvotes</th>\n",
       "      <th>downvotes</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>post_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vegan hacktivists looking developers ui design...</td>\n",
       "      <td>1</td>\n",
       "      <td>r/vegan</td>\n",
       "      <td>veganactivismbot</td>\n",
       "      <td>76</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>f3svif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>last words fellow vegan elijah mcclain murdere...</td>\n",
       "      <td>1</td>\n",
       "      <td>r/vegan</td>\n",
       "      <td>VenmoMeFiveBucks</td>\n",
       "      <td>5114</td>\n",
       "      <td>0</td>\n",
       "      <td>409</td>\n",
       "      <td>hf6eej</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>promising future think</td>\n",
       "      <td>1</td>\n",
       "      <td>r/vegan</td>\n",
       "      <td>The_Shorey</td>\n",
       "      <td>2709</td>\n",
       "      <td>0</td>\n",
       "      <td>151</td>\n",
       "      <td>hfkmlc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  vegan_label subred_name  \\\n",
       "0  vegan hacktivists looking developers ui design...            1     r/vegan   \n",
       "1  last words fellow vegan elijah mcclain murdere...            1     r/vegan   \n",
       "2                             promising future think            1     r/vegan   \n",
       "\n",
       "             author  upvotes  downvotes  num_comments post_id  \n",
       "0  veganactivismbot       76          0             0  f3svif  \n",
       "1  VenmoMeFiveBucks     5114          0           409  hf6eej  \n",
       "2        The_Shorey     2709          0           151  hfkmlc  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop Downvotes Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2067, 7)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#since downvotes column is completely empty, we will drop the column\n",
    "all_data_df.drop(columns='downvotes',axis=1,inplace=True)\n",
    "all_data_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create wourd_count column\n",
    "\n",
    "We will be using this for EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "184"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create a feature for word count per post\n",
    "all_data_df['word_count']=all_data_df['text'].apply(lambda x: len(x.split(\" \")))\n",
    "\n",
    "#check that the column data is accurate\n",
    "all_data_df.loc[0,'word_count']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check for null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text            0\n",
       "vegan_label     0\n",
       "subred_name     0\n",
       "author          0\n",
       "upvotes         0\n",
       "num_comments    0\n",
       "post_id         0\n",
       "word_count      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data_df.isnull().sum()\n",
    "#seems like there arent any null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>vegan_label</th>\n",
       "      <th>subred_name</th>\n",
       "      <th>author</th>\n",
       "      <th>upvotes</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>post_id</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>794</th>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>r/vegan</td>\n",
       "      <td>48151_62342</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>hc8dgw</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    text  vegan_label subred_name       author  upvotes  num_comments post_id  \\\n",
       "794                 1     r/vegan  48151_62342        7             0  hc8dgw   \n",
       "\n",
       "     word_count  \n",
       "794           1  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's take a look at empty strings, just in case\n",
    "all_data_df[all_data_df['text']==\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2066, 8)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#looks like there is a row with an empty text field\n",
    "#since there is no way that we can accurately impute text data,\n",
    "#we will be dropping that row\n",
    "\n",
    "#we will first convert the \"\" to None and dropna\n",
    "all_data_df.drop(all_data_df.index[all_data_df['text']==\"\"],axis=0,inplace=True)\n",
    "all_data_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have no more null values, let's check for moderator posts and duplicates, which regularly happens when scrapping reddit data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check for moderator bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text            18\n",
       "vegan_label     18\n",
       "subred_name     18\n",
       "author          18\n",
       "upvotes         18\n",
       "num_comments    18\n",
       "post_id         18\n",
       "word_count      18\n",
       "dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#since moderator posts are not actual input from our 'survey group' and \n",
    "#may skew our data, we consider them to be outliers\n",
    "#thus, we will be removing them\n",
    "all_data_df[all_data_df['author']=='AutoModerator'].count()\n",
    "\n",
    "#we observe 16 posts from moderators and will be dropping these rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check for posts by deleted users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text            4\n",
       "vegan_label     4\n",
       "subred_name     4\n",
       "author          4\n",
       "upvotes         4\n",
       "num_comments    4\n",
       "post_id         4\n",
       "word_count      4\n",
       "dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data_df[all_data_df['author']=='[deleted]'].count()\n",
    "\n",
    "#we observe 4 posts that were deleted but we will leave it in\n",
    "#the data should still be valid even though the account was deleted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2048, 8)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data_df = all_data_df[~all_data_df['author'].isin(['AutoModerator'])]\n",
    "all_data_df.shape\n",
    "#dropped 16 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check for duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1644"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_data_df['text'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1644, 8)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data_df.drop_duplicates(subset=['text'],inplace=True)\n",
    "all_data_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    0.587591\n",
      "0    0.412409\n",
      "Name: vegan_label, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(all_data_df['vegan_label'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation of text data split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the value_counts, we can see that our text data from the various subreddit are split at a 58/42 split. This will be important when we are performing EDA and comparing between the 2 subreddits. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Data to CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will be saving all my dataframes into csv files, for ease of access in the EDA phase. \n",
    "\n",
    "Here is a list of the csv files, followed by the description \n",
    "1. vegan_clean -> clean text data and label from the vegan subreddit, before dropping null and duplicates\n",
    "2. keto_clean -> clean text data and label from the keto subreddit, before dropping null and duplicates\n",
    "3. data_clean -> all relevant data of posts from both subreddits, after dropping null and duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vegan_df.to_csv('../datasets/vegan_clean.csv',index=False)\n",
    "keto_df.to_csv('../datasets/keto_clean.csv',index=False)\n",
    "all_data_df.to_csv('../datasets/data_clean.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next Steps:\n",
    "Having now scapped our data, we will now proceed to explore our data in the EDA notebook.\n",
    "\n",
    "- [EDA](./book2_eda.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
