{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3 - Book 1: Web APIs & Classification\n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "Nutrino is a leading provider of nutrition related data services and analytics. Nutrino works with businesses and professionals to improve the success of their products and programs, better understand populations and eating patterns, and identify new areas of opportunity. As part of the data science team, we are tasked to generate business ready data to support the company. To do so, we will be utilising classification models such as Logistic Regression and Naive Bayes to uncover patterns within 2 popular diets, Keto and Vegan. We hope to reveal and identify previously unrecognised sub-trends that pertains to attitudes, lifestyles and buying behavior, to allow our customers to identify strong sub trends as opposed to passing sub trends.\n",
    "\n",
    "## Executive Summary\n",
    "\n",
    "We have been tasked to predict housing prices so as to generate actionable insights for the organisation to achieve larger margins in their investment strategy. In order to achieve our goals, we will be performing data cleaning, feature engineering, EDA, feature selection and lastly several regression models to predict sale prices. Based on an accuracy score, the best model will be evaluated and chosen to predict sale prices. Having mirrored the market, we can then find out which are the strong predictors of sale prices. With this information, the company is able to locate properties with the favoured features and flip them for profit, generating value for the management, shareholders and of course customers. \n",
    "\n",
    "## Notebooks:\n",
    "- [Data Scrapping and Cleaning](./book1_data_scrapping_cleaning.ipynb)\n",
    "- [EDA and Feature Selection](./book2_eda_feature_selection.ipynb)\n",
    "- [Preprocessing, Modeling and Recommendations](./book3_preprocesing_modeling_recommendations.ipynb)\n",
    "\n",
    "## Contents:\n",
    "- [Import Libraries](#Import-Libraries)\n",
    "- [Data Scrapping](#Data-Scrapping)\n",
    "- [Data Cleaning](#Data-Cleaning)\n",
    "- [Feature Engineering](#Feature-Engineering)\n",
    "- [Save Data to CSV](#Save-Data-to-CSV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import random\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Scrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get request\n",
    "#res = requests.get(url,headers=header)\n",
    "#check status code\n",
    "#res.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#set header so that reddit wont think we are a bot and block us\n",
    "header = {'User-agent':'ididitforthemulz'}\n",
    "\n",
    "#list of subreddits that we want to scrap\n",
    "sub_reds = ['vegan','keto']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "https://old.reddit.com/r/vegan.json\n",
      "30\n",
      "1\n",
      "https://old.reddit.com/r/vegan.json?after=t3_hcyr7b\n",
      "15\n",
      "2\n",
      "https://old.reddit.com/r/vegan.json?after=t3_hd6zba\n",
      "34\n",
      "3\n",
      "https://old.reddit.com/r/vegan.json?after=t3_hcmw2v\n",
      "20\n",
      "4\n",
      "https://old.reddit.com/r/vegan.json?after=t3_hcytnj\n",
      "11\n",
      "5\n",
      "https://old.reddit.com/r/vegan.json?after=t3_hcxvy4\n",
      "18\n",
      "6\n",
      "https://old.reddit.com/r/vegan.json?after=t3_hcei9g\n",
      "44\n",
      "7\n",
      "https://old.reddit.com/r/vegan.json?after=t3_hcbj7v\n",
      "29\n",
      "8\n",
      "https://old.reddit.com/r/vegan.json?after=t3_hc3gyi\n",
      "30\n",
      "9\n",
      "https://old.reddit.com/r/vegan.json?after=t3_hbtswx\n",
      "44\n",
      "10\n",
      "https://old.reddit.com/r/vegan.json?after=t3_hbsn5n\n",
      "15\n",
      "11\n",
      "https://old.reddit.com/r/vegan.json?after=t3_hc3jet\n",
      "33\n",
      "12\n",
      "https://old.reddit.com/r/vegan.json?after=t3_hc33am\n",
      "35\n",
      "13\n",
      "https://old.reddit.com/r/vegan.json?after=t3_hbixs0\n",
      "46\n",
      "14\n",
      "https://old.reddit.com/r/vegan.json?after=t3_hascvj\n",
      "36\n",
      "15\n",
      "https://old.reddit.com/r/vegan.json?after=t3_hbngp0\n",
      "19\n",
      "16\n",
      "https://old.reddit.com/r/vegan.json?after=t3_hbmb0z\n",
      "14\n",
      "17\n",
      "https://old.reddit.com/r/vegan.json?after=t3_havjal\n",
      "49\n",
      "18\n",
      "https://old.reddit.com/r/vegan.json?after=t3_hb72yc\n",
      "30\n",
      "19\n",
      "https://old.reddit.com/r/vegan.json?after=t3_hbcxza\n",
      "11\n",
      "20\n",
      "https://old.reddit.com/r/vegan.json?after=t3_haqozk\n",
      "3\n",
      "21\n",
      "https://old.reddit.com/r/vegan.json?after=t3_haui9m\n",
      "21\n",
      "22\n",
      "https://old.reddit.com/r/vegan.json?after=t3_hazo4h\n",
      "41\n",
      "23\n",
      "https://old.reddit.com/r/vegan.json?after=t3_hav0lm\n",
      "49\n",
      "24\n",
      "https://old.reddit.com/r/vegan.json?after=t3_haxszk\n",
      "34\n",
      "25\n",
      "https://old.reddit.com/r/vegan.json?after=t3_haj1q1\n",
      "25\n",
      "26\n",
      "https://old.reddit.com/r/vegan.json?after=t3_ha13s4\n",
      "24\n",
      "27\n",
      "https://old.reddit.com/r/vegan.json?after=t3_ha26ri\n",
      "16\n",
      "28\n",
      "https://old.reddit.com/r/vegan.json?after=t3_ha0coq\n",
      "6\n",
      "29\n",
      "https://old.reddit.com/r/vegan.json?after=t3_h9rudb\n",
      "18\n",
      "30\n",
      "https://old.reddit.com/r/vegan.json?after=t3_h9jm4p\n",
      "19\n",
      "31\n",
      "https://old.reddit.com/r/vegan.json?after=t3_h9059d\n",
      "34\n",
      "32\n",
      "https://old.reddit.com/r/vegan.json?after=t3_h9j4tu\n",
      "6\n",
      "33\n",
      "https://old.reddit.com/r/vegan.json?after=t3_h93oyb\n",
      "19\n",
      "34\n",
      "https://old.reddit.com/r/vegan.json?after=t3_h917pn\n",
      "36\n",
      "35\n",
      "https://old.reddit.com/r/vegan.json?after=t3_h8tccj\n",
      "32\n",
      "36\n",
      "https://old.reddit.com/r/vegan.json?after=t3_h8ywlb\n",
      "9\n",
      "37\n",
      "https://old.reddit.com/r/vegan.json?after=t3_h90arm\n",
      "28\n",
      "38\n",
      "https://old.reddit.com/r/vegan.json?after=t3_h8wb1x\n",
      "34\n",
      "39\n",
      "https://old.reddit.com/r/vegan.json\n",
      "26\n",
      "40\n",
      "https://old.reddit.com/r/vegan.json?after=t3_hcyr7b\n",
      "42\n",
      "41\n",
      "https://old.reddit.com/r/vegan.json?after=t3_hcudyl\n",
      "22\n",
      "0\n",
      "https://old.reddit.com/r/keto.json?after=t3_hcnctm\n",
      "18\n",
      "1\n",
      "https://old.reddit.com/r/keto.json\n",
      "28\n",
      "2\n",
      "https://old.reddit.com/r/keto.json?after=t3_hcwtv2\n",
      "42\n",
      "3\n",
      "https://old.reddit.com/r/keto.json?after=t3_hcshm0\n",
      "50\n",
      "4\n",
      "https://old.reddit.com/r/keto.json?after=t3_hc0edm\n",
      "48\n",
      "5\n",
      "https://old.reddit.com/r/keto.json?after=t3_hc9y14\n",
      "11\n",
      "6\n",
      "https://old.reddit.com/r/keto.json?after=t3_hbvky1\n",
      "27\n",
      "7\n",
      "https://old.reddit.com/r/keto.json?after=t3_hbqnpz\n",
      "28\n",
      "8\n",
      "https://old.reddit.com/r/keto.json?after=t3_hbidxo\n",
      "15\n",
      "9\n",
      "https://old.reddit.com/r/keto.json?after=t3_hb6cek\n",
      "37\n",
      "10\n",
      "https://old.reddit.com/r/keto.json?after=t3_hb284n\n",
      "20\n",
      "11\n",
      "https://old.reddit.com/r/keto.json?after=t3_hamh6p\n",
      "26\n",
      "12\n",
      "https://old.reddit.com/r/keto.json?after=t3_ha5rfl\n",
      "46\n",
      "13\n",
      "https://old.reddit.com/r/keto.json?after=t3_ha6nx6\n",
      "40\n",
      "14\n",
      "https://old.reddit.com/r/keto.json?after=t3_ha10gp\n",
      "48\n",
      "15\n",
      "https://old.reddit.com/r/keto.json?after=t3_h9pyoc\n",
      "50\n",
      "16\n",
      "https://old.reddit.com/r/keto.json?after=t3_h9dhq9\n",
      "10\n",
      "17\n",
      "https://old.reddit.com/r/keto.json?after=t3_h93ih4\n",
      "47\n",
      "18\n",
      "https://old.reddit.com/r/keto.json?after=t3_h8zdfe\n",
      "16\n",
      "19\n",
      "https://old.reddit.com/r/keto.json?after=t3_h8ht8u\n",
      "14\n",
      "20\n",
      "https://old.reddit.com/r/keto.json?after=t3_h8g9oq\n",
      "46\n",
      "21\n",
      "https://old.reddit.com/r/keto.json?after=t3_h7lkca\n",
      "47\n",
      "22\n",
      "https://old.reddit.com/r/keto.json?after=t3_h16s0v\n",
      "6\n",
      "23\n",
      "https://old.reddit.com/r/keto.json?after=t3_h7ckfw\n",
      "22\n",
      "24\n",
      "https://old.reddit.com/r/keto.json?after=t3_h7ijql\n",
      "15\n",
      "25\n",
      "https://old.reddit.com/r/keto.json?after=t3_h16etq\n",
      "41\n",
      "26\n",
      "https://old.reddit.com/r/keto.json?after=t3_h11f18\n",
      "34\n",
      "27\n",
      "https://old.reddit.com/r/keto.json?after=t3_gzzyvr\n",
      "18\n",
      "28\n",
      "https://old.reddit.com/r/keto.json?after=t3_h0m3wg\n",
      "40\n",
      "29\n",
      "https://old.reddit.com/r/keto.json?after=t3_h0ads7\n",
      "42\n",
      "30\n",
      "https://old.reddit.com/r/keto.json?after=t3_gzqakd\n",
      "47\n",
      "31\n",
      "https://old.reddit.com/r/keto.json\n",
      "13\n",
      "32\n",
      "https://old.reddit.com/r/keto.json?after=t3_hcwtv2\n",
      "7\n",
      "33\n",
      "https://old.reddit.com/r/keto.json?after=t3_hcshm0\n",
      "31\n",
      "34\n",
      "https://old.reddit.com/r/keto.json?after=t3_hc0edm\n",
      "9\n",
      "35\n",
      "https://old.reddit.com/r/keto.json?after=t3_hc9y14\n",
      "26\n",
      "36\n",
      "https://old.reddit.com/r/keto.json?after=t3_hbvky1\n",
      "50\n",
      "37\n",
      "https://old.reddit.com/r/keto.json?after=t3_hbqnpz\n",
      "29\n",
      "38\n",
      "https://old.reddit.com/r/keto.json?after=t3_hbidxo\n",
      "8\n",
      "39\n",
      "https://old.reddit.com/r/keto.json?after=t3_hb6cek\n",
      "47\n",
      "40\n",
      "https://old.reddit.com/r/keto.json?after=t3_hb284n\n",
      "5\n",
      "41\n",
      "https://old.reddit.com/r/keto.json?after=t3_hamh6p\n",
      "44\n",
      "CPU times: user 2.27 s, sys: 538 ms, total: 2.81 s\n",
      "Wall time: 40min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#scrap vegan and keto posts \n",
    "all_posts = []\n",
    "after = None\n",
    "\n",
    "for sub in sub_reds:\n",
    "    url = f'https://old.reddit.com/r/{sub}.json'\n",
    "    posts = []\n",
    "    \n",
    "    for a in range(42):\n",
    "        print(a)\n",
    "        if after == None:\n",
    "            current_url = url\n",
    "        else:\n",
    "            current_url = url + '?after=' + after\n",
    "        print(current_url)\n",
    "        res = requests.get(current_url, headers=header)\n",
    "\n",
    "        if res.status_code != 200:\n",
    "            print('Status error', res.status_code)\n",
    "            break\n",
    "\n",
    "        current_dict = res.json()\n",
    "        current_posts = [p['data'] for p in current_dict['data']['children']]\n",
    "        posts.extend(current_posts)\n",
    "        after = current_dict['data']['after']\n",
    "\n",
    "        # generate a random sleep duration to look more 'natural'\n",
    "        sleep_duration = random.randint(3,50)\n",
    "        print(sleep_duration)\n",
    "        time.sleep(sleep_duration)\n",
    "        \n",
    "    all_posts.append(posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Is eating masala mixtures ok for a keto diet.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check to find title of scrapped json data\n",
    "res.json()['data']['children'][0]['data']['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Living in an Indian household, a lot of our foods have masalas ( coriander powder, chilli powder, turmeric, cumin powder, coconut seeds etc) in them mixed in with certain meats. \\n\\nI wanted to know if eating that would affect my diet.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check to find text of scrapped json data\n",
    "res.json()['data']['children'][0]['data']['selftext']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jazzjj5864'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.json()['data']['children'][0]['data']['author']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['approved_at_utc', 'subreddit', 'selftext', 'author_fullname', 'saved', 'mod_reason_title', 'gilded', 'clicked', 'title', 'link_flair_richtext', 'subreddit_name_prefixed', 'hidden', 'pwls', 'link_flair_css_class', 'downs', 'thumbnail_height', 'top_awarded_type', 'hide_score', 'name', 'quarantine', 'link_flair_text_color', 'upvote_ratio', 'author_flair_background_color', 'subreddit_type', 'ups', 'total_awards_received', 'media_embed', 'thumbnail_width', 'author_flair_template_id', 'is_original_content', 'user_reports', 'secure_media', 'is_reddit_media_domain', 'is_meta', 'category', 'secure_media_embed', 'link_flair_text', 'can_mod_post', 'score', 'approved_by', 'author_premium', 'thumbnail', 'edited', 'author_flair_css_class', 'author_flair_richtext', 'gildings', 'content_categories', 'is_self', 'mod_note', 'created', 'link_flair_type', 'wls', 'removed_by_category', 'banned_by', 'author_flair_type', 'domain', 'allow_live_comments', 'selftext_html', 'likes', 'suggested_sort', 'banned_at_utc', 'view_count', 'archived', 'no_follow', 'is_crosspostable', 'pinned', 'over_18', 'all_awardings', 'awarders', 'media_only', 'link_flair_template_id', 'can_gild', 'spoiler', 'locked', 'author_flair_text', 'treatment_tags', 'visited', 'removed_by', 'num_reports', 'distinguished', 'subreddit_id', 'mod_reason_by', 'removal_reason', 'link_flair_background_color', 'id', 'is_robot_indexable', 'report_reasons', 'author', 'discussion_type', 'num_comments', 'send_replies', 'whitelist_status', 'contest_mode', 'mod_reports', 'author_patreon_flair', 'author_flair_text_color', 'permalink', 'parent_whitelist_status', 'stickied', 'url', 'subreddit_subscribers', 'created_utc', 'num_crossposts', 'media', 'is_video'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.json()['data']['children'][0]['data'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_lst = []\n",
    "title_lst = []\n",
    "subred_name_lst = []\n",
    "ups_lst = []\n",
    "downs_lst = []\n",
    "num_comments_lst = []\n",
    "id_lst = []\n",
    "author_lst = []\n",
    "\n",
    "for sub_red in all_posts:\n",
    "    for i in range(len(sub_red)):\n",
    "        text_lst.append(sub_red[i]['selftext'])\n",
    "        title_lst.append(sub_red[i]['title'])\n",
    "        subred_name_lst.append(sub_red[i]['subreddit_name_prefixed'])\n",
    "        ups_lst.append(sub_red[i]['ups'])\n",
    "        downs_lst.append(sub_red[i]['downs'])\n",
    "        num_comments_lst.append(sub_red[i]['num_comments'])\n",
    "        id_lst.append(sub_red[i]['id'])\n",
    "        author_lst.append(sub_red[i]['author'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_posts_df = pd.DataFrame(zip(text_lst,title_lst,subred_name_lst,\n",
    "                                ups_lst,downs_lst,num_comments_lst,\n",
    "                                id_lst,author_lst),\n",
    "                           columns=['text','title','label','ups','downs','num_comments','id','author'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "      <th>label</th>\n",
       "      <th>ups</th>\n",
       "      <th>downs</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>id</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>Vegan Hacktivists are looking for professional...</td>\n",
       "      <td>r/vegan</td>\n",
       "      <td>208</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>feve92</td>\n",
       "      <td>veganactivismbot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td>Regan Russell, animal rights activist. She was...</td>\n",
       "      <td>r/vegan</td>\n",
       "      <td>5097</td>\n",
       "      <td>0</td>\n",
       "      <td>485</td>\n",
       "      <td>hca93z</td>\n",
       "      <td>nekkototoro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td>Celebrating 24 years of veganism this month wi...</td>\n",
       "      <td>r/vegan</td>\n",
       "      <td>2665</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>hcvo0c</td>\n",
       "      <td>Esmeanne</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>My sister just visited our dad and extended fa...</td>\n",
       "      <td>\"Vegans are preachy and shove it in your face\"</td>\n",
       "      <td>r/vegan</td>\n",
       "      <td>86</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>hd4exc</td>\n",
       "      <td>Elemor_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "      <td>Farmers: veganism is propaganda. Also farmers:...</td>\n",
       "      <td>r/vegan</td>\n",
       "      <td>3391</td>\n",
       "      <td>0</td>\n",
       "      <td>252</td>\n",
       "      <td>hcmghs</td>\n",
       "      <td>nekkototoro</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0                                                      \n",
       "1                                                      \n",
       "2                                                      \n",
       "3  My sister just visited our dad and extended fa...   \n",
       "4                                                      \n",
       "\n",
       "                                               title    label   ups  downs  \\\n",
       "0  Vegan Hacktivists are looking for professional...  r/vegan   208      0   \n",
       "1  Regan Russell, animal rights activist. She was...  r/vegan  5097      0   \n",
       "2  Celebrating 24 years of veganism this month wi...  r/vegan  2665      0   \n",
       "3     \"Vegans are preachy and shove it in your face\"  r/vegan    86      0   \n",
       "4  Farmers: veganism is propaganda. Also farmers:...  r/vegan  3391      0   \n",
       "\n",
       "   num_comments      id            author  \n",
       "0             1  feve92  veganactivismbot  \n",
       "1           485  hca93z       nekkototoro  \n",
       "2           120  hcvo0c          Esmeanne  \n",
       "3            24  hd4exc           Elemor_  \n",
       "4           252  hcmghs       nekkototoro  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_posts_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_posts_df.to_csv('../datasets/all_posts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Canada is about to pass a bill that would make...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Vegan Hacktivists are looking for professional...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Regan Russell, animal rights activist. She was...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>She do be spitting straight facts tho.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>When there's another post about deforestation ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title\n",
       "0  Canada is about to pass a bill that would make...\n",
       "1  Vegan Hacktivists are looking for professional...\n",
       "2  Regan Russell, animal rights activist. She was...\n",
       "3             She do be spitting straight facts tho.\n",
       "4  When there's another post about deforestation ..."
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#vegan_df=pd.DataFrame(zip(text_lst,title_lst),columns=['text','title'])\n",
    "vegan_lst=pd.DataFrame(title_lst,columns=['title'])\n",
    "vegan_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vegan_df.to_csv('vegan_posts.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### First observations:\n",
    "- Now we have all our title data from the subreddit\n",
    "- There are html syntax observed. We need to remove it.\n",
    "- We will also convert the data to lowercase and remove stop words\n",
    "- and finally, we will lemmatise the words instead of stemming which is a harsher method and could produce some non-words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have collected our data, we will move to cleaning the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Reprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#define a method to preprocess data\n",
    "def robust_text_preprocessing(text):\n",
    "    #change to lower\n",
    "    text=text.lower()\n",
    "    \n",
    "    #remove punctuation\n",
    "    text=re.sub(r\"[^A-Za-z0-9]\",\" \",text)\n",
    "    \n",
    "    #lemmatize\n",
    "    lemmatizer=WordNetLemmatizer()\n",
    "    text=lemmatizer.lemmatize(text)\n",
    "    \n",
    "    #remove stopwords\n",
    "    words=text.split()\n",
    "    text= [word for word in words if word not in stopwords.words('english')]\n",
    "    text=(\" \".join(text))\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning and parsing the Vegan posts...\n",
      "Review 100 of 999.\n",
      "Review 200 of 999.\n",
      "Review 300 of 999.\n",
      "Review 400 of 999.\n",
      "Review 500 of 999.\n",
      "Review 600 of 999.\n",
      "Review 700 of 999.\n",
      "Review 800 of 999.\n",
      "Review 900 of 999.\n",
      "CPU times: user 1.4 s, sys: 288 ms, total: 1.69 s\n",
      "Wall time: 1.69 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "vegan_clean=[]\n",
    "print(\"Cleaning and parsing the Vegan posts...\")\n",
    "\n",
    "j = 0\n",
    "for str in vegan_df['title']:\n",
    "    # Convert review to words, then append to clean_train_reviews.\n",
    "    vegan_clean.append(robust_text_preprocessing(str))\n",
    "    \n",
    "    # If the index is divisible by 1000, print a message\n",
    "    if (j + 1) % 100 == 0:\n",
    "        print(f'Review {j + 1} of {vegan_df.shape[0]}.')\n",
    "    \n",
    "    j += 1\n",
    "\n",
    "# Let's do the same for our Keto data.\n",
    "# X_test_clean=[]\n",
    "# print(\"Cleaning and parsing the Keto posts...\")\n",
    "\n",
    "# j = 0\n",
    "# for str in X_test:\n",
    "#     # Convert review to words, then append to clean_train_reviews.\n",
    "#     X_test_clean.append(robust_text_preprocessing(str))\n",
    "    \n",
    "#     # If the index is divisible by 1000, print a message\n",
    "#     if (j + 1) % 100 == 0:\n",
    "#         print(f'Review {j + 1} of {len(X_train)}.')\n",
    "    \n",
    "#     j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'canada pass bill would make illegal report photograph otherwise expose animal abuse factory farms already know animals endure silences even worst cases torture abuse please sign petition'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vegan_clean[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vegan_label = [1 if name == \"r/vegan\" else 0 for name in subred_name ]\n",
    "print(len(vegan_label))\n",
    "print(vegan_label[:5])\n",
    "print(vegan_label[-5:])#here we create a label for individual df for each subreddit\n",
    "vegan_label = [1 for str in vegan_posts]\n",
    "print(len(vegan_label))\n",
    "print(vegan_label[:5])\n",
    "keto_label = [0 for str in keto_posts]\n",
    "print(len(keto_label))\n",
    "print(keto_label[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>canada pass bill would make illegal report pho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>vegan hacktivists looking professional develop...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>regan russell animal rights activist killed st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>spitting straight facts tho</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>another post deforestation amazon front page</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title\n",
       "0  canada pass bill would make illegal report pho...\n",
       "1  vegan hacktivists looking professional develop...\n",
       "2  regan russell animal rights activist killed st...\n",
       "3                        spitting straight facts tho\n",
       "4       another post deforestation amazon front page"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check to see if we have properly cleaned the data\n",
    "vegan_clean_df=pd.DataFrame(vegan_clean,columns=['title'])\n",
    "#vegan_clean_df['vegan']=1\n",
    "vegan_clean_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Decisions for null values\n",
    "\n",
    "- For columns with extremely high numbers of nan such as alley, pool and misc_feature, we will be dropping these columns as the lack of data will be statistically insignificant and lead to dimensonality error/bias\n",
    "- For mas_vnr_xxx/garage_xx/bsmt_xx, it seems like there are missing values because there is no such feature in the property in the first place. We will replace nan with 0 for these variables. \n",
    "- For variables such as garage_yr built, we will impute it with the year the house was built which is the norm for most houses with a garage\n",
    "- Lastly, for continuous variables such as lot frontage, we will be imputing with mean values based on the lot shape and lot area\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropping columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imput NaNs with 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's get to cleaning!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on some external research, we have identified some factors that are comonnly known to affect the price of a property\n",
    "- Location\n",
    "- Property size\n",
    "- House condition\n",
    "- Macro Environment\n",
    "\n",
    "3rd party research: https://resources.point.com/8-biggest-factors-affect-real-estate-prices/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Location, Location, Location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to facilitate our EDA process and allow the model to read the words, we will convert the our words into a document term matrix. We will do so with a CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#instantiate a CountVectorizer\n",
    "# cvec = CountVectorizer()\n",
    "\n",
    "#convert the data into a sparse matrix\n",
    "# posts_cvec = cvec.fit_transform(all_data_df['text'])\n",
    "# text_cvec_df = pd.DataFrame(posts_cvec.toarray(),columns=cvec.get_feature_names())\n",
    "# text_cvec_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Data to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ames.to_csv('../datasets/combined_clean.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next Steps:\n",
    "Having now gotten most of the features that we need to address the issues, we will now proceed to establishing a baseline for our model in the preprocessing and modeling notebook.\n",
    "\n",
    "- [EDA and Feature Selection](./book2_eda_feature_selection.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for coding use only - DELETE BEFORE SUBMISSION\n",
    "vegan_df=pd.read_csv('../datasets/vegan_clean.csv')\n",
    "keto_df=pd.read_csv('../datasets/keto_clean.csv')\n",
    "posts_df=pd.read_csv('../datasets/all_posts_clean.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we split our data into train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#first, we define our feature matrix and the target\n",
    "X = posts_df['text']\n",
    "y = posts_df['vegan_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
